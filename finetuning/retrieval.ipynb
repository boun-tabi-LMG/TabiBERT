{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b87668-08e5-41f5-9d4d-4710447bc9c5",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38eac79c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.1.2\n",
      "    Uninstalling huggingface_hub-1.1.2:\n",
      "      Successfully uninstalled huggingface_hub-1.1.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.4\n",
      "    Uninstalling tokenizers-0.21.4:\n",
      "      Successfully uninstalled tokenizers-0.21.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.3\n",
      "    Uninstalling transformers-4.53.3:\n",
      "      Successfully uninstalled transformers-4.53.3\n",
      "Successfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cde8efd7-00e3-4a3e-b753-61f7e684bed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (1.2.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e753a723-8df8-4086-a7cf-4af1dd50c569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b91bbed-4f0e-47db-a619-f79b58d47c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub<1.0.0 in /usr/local/lib/python3.11/dist-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0.0) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0.0) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0.0) (2024.8.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"huggingface_hub<1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe53ac1-f935-4d77-b80b-24765b4a4d07",
   "metadata": {},
   "source": [
    "# INITIALIZING the Retrival Task Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8680797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d25e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_remove_none(dataset, text_column_names):\n",
    "    \"\"\"\n",
    "    Checks for and removes rows with None values in specified columns.\n",
    "\n",
    "    Args:\n",
    "        dataset (datasets.Dataset): The dataset to process.\n",
    "        text_column_names (list): List of column names to check for None values.\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: The processed dataset with None rows removed.\n",
    "    \"\"\"\n",
    "    indices_to_keep = []\n",
    "    for i in range(len(dataset)):\n",
    "        is_valid = True\n",
    "        for column_name in text_column_names:\n",
    "            if dataset[i][column_name] is None:\n",
    "                is_valid = False\n",
    "                break\n",
    "        if is_valid:\n",
    "            indices_to_keep.append(i)\n",
    "    return dataset.select(indices_to_keep)\n",
    "\n",
    "\n",
    "def load_retrieval_dataset(retrieval_task_id: int):\n",
    "    if retrieval_task_id == 0:\n",
    "        dataset = load_dataset(\"trmteb/wmt16_en_tr_fine_tuning_dataset\")\n",
    "        # splits: train, validation, test\n",
    "    elif retrieval_task_id == 1:\n",
    "        dataset = load_dataset(\"trmteb/msmarco-tr_fine_tuning_dataset\")\n",
    "        # splits: train, dev, test\n",
    "    elif retrieval_task_id == 2:\n",
    "        dataset = load_dataset(\"trmteb/scifact-tr_fine_tuning_dataset\")\n",
    "        train_dev = dataset['train'].train_test_split(test_size=0.36, seed=25)\n",
    "        train = train_dev['train']\n",
    "        dev = train_dev['test']\n",
    "        dataset['train'] = train\n",
    "        dataset['validation'] = dev\n",
    "        # splits: train, validation, test\n",
    "    elif retrieval_task_id == 3:\n",
    "        dataset = load_dataset(\"trmteb/fiqa-tr_fine_tuning_dataset\")\n",
    "        # splits: train, dev, test\n",
    "    elif retrieval_task_id == 4:\n",
    "        dataset = load_dataset(\"trmteb/nfcorpus-tr_fine_tuning_dataset\")\n",
    "        # splits: train, dev, test\n",
    "    elif retrieval_task_id == 5:\n",
    "        dataset = load_dataset(\"trmteb/quora-tr_fine_tuning_dataset\")\n",
    "        train_dev = dataset['dev'].train_test_split(test_size=0.2, seed=25)\n",
    "        train = train_dev['train']\n",
    "        dev = train_dev['test']\n",
    "        dataset['train'] = train\n",
    "        dataset['validation'] = dev\n",
    "    if \"validation\" not in dataset and \"dev\" in dataset:\n",
    "        dataset[\"validation\"] = dataset.pop(\"dev\")\n",
    "    # print(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb1ad25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    dev: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 7626\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 15675\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 6100\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 1526\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(load_retrieval_dataset(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751e24eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 0\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 205756\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 1001\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "========================================\n",
      "Loading dataset 1\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 253332\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 31692\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 31540\n",
      "    })\n",
      "})\n",
      "========================================\n",
      "Loading dataset 2\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 588\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 339\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 331\n",
      "    })\n",
      "})\n",
      "========================================\n",
      "Loading dataset 3\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 14166\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 1706\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 1238\n",
      "    })\n",
      "})\n",
      "========================================\n",
      "Loading dataset 4\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 110575\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 12334\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 11385\n",
      "    })\n",
      "})\n",
      "========================================\n",
      "Loading dataset 5\n",
      "DatasetDict({\n",
      "    dev: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 7626\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive'],\n",
      "        num_rows: 15675\n",
      "    })\n",
      "})\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"Loading dataset {i}\")\n",
    "    load_retrieval_dataset(i)\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7959c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval tasks quantile values:\n",
    "multiple_of_value = 128\n",
    "\n",
    "\n",
    "def determine_quantile_values_retrieval(\n",
    "        ds,\n",
    "        split=\"train\",\n",
    "        anchor_col=\"anchor\",\n",
    "        positive_col=\"positive\",\n",
    "        tokenizer=None,\n",
    "        quantiles=(0.95, 0.98, 0.99),\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute and print length quantiles for retrieval datasets consisting of\n",
    "    anchor/positive text pairs.\n",
    "\n",
    "    Args:\n",
    "        ds (datasets.DatasetDict): Dataset with anchor/positive columns.\n",
    "        split (str): Which split to analyze (default: \"train\").\n",
    "        anchor_col (str): Column name for the anchor text.\n",
    "        positive_col (str): Column name for the positive text.\n",
    "        tokenizer (AutoTokenizer, optional): Tokenizer to use. Defaults to\n",
    "            the globally defined `tabibert_tokenizer`.\n",
    "        quantiles (tuple): Quantiles to report, defaults to (.95, .98, .99).\n",
    "\n",
    "    Returns:\n",
    "        dict: Nested dict of quantile stats for anchor, positive and pair lengths.\n",
    "    \"\"\"\n",
    "    tok = tokenizer or tabibert_tokenizer\n",
    "    split_dataset = ds[split]\n",
    "    split_dataset = check_and_remove_none(split_dataset,\n",
    "                                          [anchor_col, positive_col])\n",
    "    def _clean(text):\n",
    "        if text is None:\n",
    "            return None\n",
    "        if isinstance(text, str):\n",
    "            return text.replace(\"\\n\", \" \").strip()\n",
    "        return str(text)\n",
    "\n",
    "    anchor_texts = [\n",
    "        _clean(example.get(anchor_col)) for example in split_dataset\n",
    "        if example.get(anchor_col)\n",
    "    ]\n",
    "    positive_texts = [\n",
    "        _clean(example.get(positive_col)) for example in split_dataset\n",
    "        if example.get(positive_col)\n",
    "    ]\n",
    "\n",
    "    if not anchor_texts or not positive_texts:\n",
    "        raise ValueError(\n",
    "            \"Dataset split does not contain valid anchor/positive texts\")\n",
    "\n",
    "    anchor_lens = np.array([len(tok.encode(text)) for text in anchor_texts])\n",
    "    positive_lens = np.array(\n",
    "        [len(tok.encode(text)) for text in positive_texts])\n",
    "    pair_lens = anchor_lens + positive_lens\n",
    "\n",
    "    def _round_up(value, multiple):\n",
    "        remainder = value % multiple\n",
    "        return value if remainder == 0 else value + multiple - remainder\n",
    "\n",
    "    def _report(name, lengths):\n",
    "        stats = {}\n",
    "        quantile_values = []\n",
    "        for q in quantiles:\n",
    "            q_value = int(np.quantile(lengths, q=q))\n",
    "            stats[f\"q{int(q*100)}\"] = q_value\n",
    "            quantile_values.append(f\"{q_value:,}\")\n",
    "        quantile_label = \" & \".join(quantile_values)\n",
    "        print(f\"{name} lengths {quantiles}: {quantile_label}\")\n",
    "        rounded = _round_up(stats[f\"q{int(quantiles[1]*100)}\"],\n",
    "                            multiple_of_value)\n",
    "        print(\n",
    "            f\"{name} recommended max length (rounded q{int(quantiles[1]*100)}): {rounded:,}\"\n",
    "        )\n",
    "        stats[\"rounded\"] = rounded\n",
    "        return stats\n",
    "\n",
    "    anchor_stats = _report(\"Anchor\", anchor_lens)\n",
    "    positive_stats = _report(\"Positive\", positive_lens)\n",
    "    pair_stats = _report(\"Pair (anchor+positive)\", pair_lens)\n",
    "\n",
    "    return {\n",
    "        \"anchor\": anchor_stats,\n",
    "        \"positive\": positive_stats,\n",
    "        \"pair\": pair_stats,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c75b43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "tabibert_tokenizer = AutoTokenizer.from_pretrained('boun-tabilab/TabiBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c5652f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Task # 0\n",
      "Anchor lengths (0.95, 0.98, 0.99): 58 & 68 & 76\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 87 & 100 & 111\n",
      "Positive recommended max length (rounded q98): 128\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 143 & 166 & 184\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 256\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 52 & 62 & 70\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 85 & 99 & 115\n",
      "Positive recommended max length (rounded q98): 128\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 137 & 161 & 179\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 256\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 53 & 63 & 70\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 80 & 95 & 104\n",
      "Positive recommended max length (rounded q98): 128\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 132 & 156 & 171\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 256\n",
      "========================================\n",
      "\n",
      "Loading Task # 1\n",
      "Anchor lengths (0.95, 0.98, 0.99): 15 & 17 & 18\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 116 & 127 & 134\n",
      "Positive recommended max length (rounded q98): 128\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 127 & 138 & 146\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 256\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 15 & 16 & 18\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 116 & 126 & 134\n",
      "Positive recommended max length (rounded q98): 128\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 126 & 137 & 145\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 256\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 15 & 17 & 18\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 116 & 127 & 134\n",
      "Positive recommended max length (rounded q98): 128\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 127 & 138 & 146\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 256\n",
      "========================================\n",
      "\n",
      "Loading Task # 2\n",
      "Anchor lengths (0.95, 0.98, 0.99): 39 & 46 & 50\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 448 & 464 & 470\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 472 & 484 & 491\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 37 & 41 & 43\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 443 & 448 & 455\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 463 & 471 & 478\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 40 & 45 & 46\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 443 & 459 & 466\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 468 & 483 & 497\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "\n",
      "Loading Task # 3\n",
      "Anchor lengths (0.95, 0.98, 0.99): 27 & 30 & 32\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 392 & 425 & 640\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 409 & 443 & 657\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 29 & 30 & 33\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 394 & 421 & 672\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 412 & 441 & 688\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 26 & 29 & 31\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 395 & 444 & 682\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 413 & 460 & 718\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "\n",
      "Loading Task # 4\n",
      "Anchor lengths (0.95, 0.98, 0.99): 17 & 19 & 21\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 449 & 466 & 472\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 456 & 475 & 484\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 19 & 22 & 26\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 449 & 466 & 472\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 457 & 476 & 485\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 17 & 18 & 22\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 450 & 466 & 472\n",
      "Positive recommended max length (rounded q98): 512\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 457 & 475 & 483\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 512\n",
      "========================================\n",
      "\n",
      "Loading Task # 5\n",
      "Anchor lengths (0.95, 0.98, 0.99): 26 & 29 & 34\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 26 & 31 & 35\n",
      "Positive recommended max length (rounded q98): 128\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 49 & 58 & 66\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 128\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 25 & 30 & 33\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 25 & 31 & 35\n",
      "Positive recommended max length (rounded q98): 128\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 48 & 57 & 65\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 128\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"Loading Task # {i}\")\n",
    "    dataset = load_retrieval_dataset(i)\n",
    "\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        if split == \"train\" and i == 5:\n",
    "            continue\n",
    "        determine_quantile_values_retrieval(\n",
    "            dataset,\n",
    "            split=split,\n",
    "            anchor_col=\"anchor\",\n",
    "            positive_col=\"positive\",\n",
    "            tokenizer=tabibert_tokenizer,\n",
    "            quantiles=(0.95, 0.98, 0.99),\n",
    "        )\n",
    "        print(\"=\" * 40)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9ce8757-f861-4198-a5ba-b2208eae3846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in boun-tabilab/Apps-TR:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3765\n",
      "    })\n",
      "})\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13619 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor lengths (0.95, 0.98, 0.99): 879 & 1,131 & 1,280\n",
      "Anchor recommended max length (rounded q98): 1,152\n",
      "Positive lengths (0.95, 0.98, 0.99): 606 & 916 & 1,217\n",
      "Positive recommended max length (rounded q98): 1,024\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 1,385 & 1,804 & 2,137\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 1,920\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 884 & 1,013 & 1,101\n",
      "Anchor recommended max length (rounded q98): 1,024\n",
      "Positive lengths (0.95, 0.98, 0.99): 743 & 1,017 & 1,316\n",
      "Positive recommended max length (rounded q98): 1,024\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 1,445 & 1,708 & 2,084\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 1,792\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d1698478b34172b595df252b1309c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/499 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bf4438d28443fea2615905379f14f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb79d9061880442d8a4d82918b71da90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/dev-00000-of-00001.parquet:   0%|          | 0.00/94.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bf824497f54d82977ba154004d2fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/90.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936c5eac963345be8cf648a17633b41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/19604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54c788fe76e407fa86d4892d1841b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24258ca9b2b410aa8c2fcb9000d8a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in boun-tabilab/CosQA-TR:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 19604\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n",
      "================================================================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 20 & 22 & 26\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 199 & 247 & 306\n",
      "Positive recommended max length (rounded q98): 256\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 215 & 264 & 321\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 384\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 20 & 23 & 25\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 210 & 234 & 324\n",
      "Positive recommended max length (rounded q98): 256\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 223 & 267 & 334\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 384\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 19 & 22 & 24\n",
      "Anchor recommended max length (rounded q98): 128\n",
      "Positive lengths (0.95, 0.98, 0.99): 197 & 223 & 248\n",
      "Positive recommended max length (rounded q98): 256\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 210 & 235 & 260\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 256\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbbf15adf09447d965746009a95460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/507 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8568e2b4716e4025b4ba41f2e989ccf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/20.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea383b50ea244ff9569182630c8f70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/dev-00000-of-00001.parquet:   0%|          | 0.00/5.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6865a602b13f4b7fae89987efc20a30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a00e034070b456abd4fb817221e4707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/13951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fbbc1a1ddf4838be3d636b661e5f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/3986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602d31993d4b4d02ae2cd88f44478d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1994 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in boun-tabilab/StackoverflowQA-TR:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 13951\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3986\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 1994\n",
      "    })\n",
      "})\n",
      "================================================================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 1,524 & 2,502 & 3,624\n",
      "Anchor recommended max length (rounded q98): 2,560\n",
      "Positive lengths (0.95, 0.98, 0.99): 1,263 & 1,915 & 2,641\n",
      "Positive recommended max length (rounded q98): 1,920\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 2,451 & 3,628 & 5,140\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 3,712\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 1,543 & 2,430 & 3,252\n",
      "Anchor recommended max length (rounded q98): 2,432\n",
      "Positive lengths (0.95, 0.98, 0.99): 1,203 & 1,816 & 2,367\n",
      "Positive recommended max length (rounded q98): 1,920\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 2,421 & 3,434 & 4,152\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 3,456\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 1,339 & 2,062 & 2,708\n",
      "Anchor recommended max length (rounded q98): 2,176\n",
      "Positive lengths (0.95, 0.98, 0.99): 1,222 & 1,931 & 2,524\n",
      "Positive recommended max length (rounded q98): 2,048\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 2,330 & 3,206 & 4,124\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 3,328\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efd5b2aee9b482f912d42d84d13eec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/505 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67d39907de54969b8984af136d1a78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/5.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa89609d6df4e39aa3d3eeea8c6db4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/dev-00000-of-00001.parquet:   0%|          | 0.00/1.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100cd085c53648fa93801b574f59e3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/1.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda79bd49424a57a65942e9e03ceba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6548081dce74560a01062414a6d9fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce2745f15da4a348606e27926c0bda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in boun-tabilab/CodeSearchNet-21K-TR:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 15000\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "================================================================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 677 & 927 & 1,090\n",
      "Anchor recommended max length (rounded q98): 1,024\n",
      "Positive lengths (0.95, 0.98, 0.99): 155 & 238 & 319\n",
      "Positive recommended max length (rounded q98): 256\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 764 & 1,021 & 1,227\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 1,024\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 688 & 935 & 1,074\n",
      "Anchor recommended max length (rounded q98): 1,024\n",
      "Positive lengths (0.95, 0.98, 0.99): 166 & 247 & 345\n",
      "Positive recommended max length (rounded q98): 256\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 779 & 1,027 & 1,276\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 1,152\n",
      "========================================\n",
      "Anchor lengths (0.95, 0.98, 0.99): 675 & 966 & 1,142\n",
      "Anchor recommended max length (rounded q98): 1,024\n",
      "Positive lengths (0.95, 0.98, 0.99): 155 & 271 & 370\n",
      "Positive recommended max length (rounded q98): 384\n",
      "Pair (anchor+positive) lengths (0.95, 0.98, 0.99): 785 & 1,035 & 1,323\n",
      "Pair (anchor+positive) recommended max length (rounded q98): 1,152\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_retrieval_datasets = [\n",
    "    \"boun-tabilab/Apps-TR\",\n",
    "    \"boun-tabilab/CosQA-TR\",\n",
    "    \"boun-tabilab/StackoverflowQA-TR\",\n",
    "    \"boun-tabilab/CodeSearchNet-21K-TR\"\n",
    "]\n",
    "\n",
    "token = \"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\"\n",
    "\n",
    "for path in code_retrieval_datasets:\n",
    "    dataset = load_dataset(\n",
    "        path,\n",
    "        # data_files={\n",
    "        #     \"train\": \"train-*.parquet\",\n",
    "        #     \"dev\": \"dev-*.parquet\",\n",
    "        #     \"test\": \"test-*.parquet\"\n",
    "        # },\n",
    "        token=token\n",
    "    )\n",
    "    print(f\"Dataset in {path}:\")\n",
    "    print(dataset)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for split in dataset:\n",
    "        determine_quantile_values_retrieval(\n",
    "            dataset,\n",
    "            split=split,\n",
    "            anchor_col=\"query\",\n",
    "            positive_col=\"doc\",\n",
    "            tokenizer=tabibert_tokenizer,\n",
    "            quantiles=(0.95, 0.98, 0.99),\n",
    "        )\n",
    "        print(\"=\" * 40)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45bcc598-33f8-42e6-9c80-383862766d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since boun-tabilab/Apps-TR couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/boun-tabilab___apps-tr/default/0.0.0/62791ffc9493a5c00238e4863a38123b9c1ed441 (last modified on Sat Nov 29 10:50:08 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 1250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3765\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3750\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since boun-tabilab/CosQA-TR couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/boun-tabilab___cos_qa-tr/default/0.0.0/6906cecc3cdd7c8b76e7e925777c9c7b9ae9bb75 (last modified on Sat Nov 29 10:57:27 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 19604\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since boun-tabilab/StackoverflowQA-TR couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/boun-tabilab___stackoverflow_qa-tr/default/0.0.0/32ac17e85a5ad86f96d2ceaebb3ce60a7f4ec009 (last modified on Sat Nov 29 10:57:40 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 13951\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3986\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 1994\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since boun-tabilab/CodeSearchNet-21K-TR couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/boun-tabilab___code_search_net-21_k-tr/default/0.0.0/b7d02d7c8d840920fb9c205771d38541baa22a73 (last modified on Sat Nov 29 10:58:12 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 15000\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'doc'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "code_retrieval_datasets = [\n",
    "        \"boun-tabilab/Apps-TR\",\n",
    "        \"boun-tabilab/CosQA-TR\",\n",
    "        \"boun-tabilab/StackoverflowQA-TR\",\n",
    "        \"boun-tabilab/CodeSearchNet-21K-TR\"\n",
    "    ]\n",
    "for code_retrieval_task_id in range(4):\n",
    "    dataset = load_dataset(\n",
    "        code_retrieval_datasets[code_retrieval_task_id],\n",
    "    )\n",
    "    if \"dev\" not in dataset:\n",
    "        train_dev_split = dataset['train'].train_test_split(test_size=0.75, seed=25)\n",
    "        dataset['train'] = train_dev_split['train']\n",
    "        dataset['dev'] = train_dev_split['test']\n",
    "\n",
    "    print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
