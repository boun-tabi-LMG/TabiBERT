model_source: "hf"
hf_model_name: YTU/BERT
model_path: ytu-ce-cosmos/turkish-base-bert-uncased
tokenizer_name: ${model_path}

max_duration: 10ep   
# max_seq_len: 512

batch_size: 32
device_eval_microbatch_size: ${batch_size}
device_train_microbatch_size: ${batch_size}
global_train_batch_size: ${batch_size}

optimizer:
  name: decoupled_adamw
  lr: 3.0e-5
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-5

is_lower: True

# wmt16:                 3.00E-05	 1.00E-05	   10	    32
# msmarco:               2.00E-05	 1.00E-05	    8	    32
# scifact:               3.00E-05	 1.00E-06	   10	    16
# quora:                 2.00E-05	 1.00E-06	   10	    32
# apps_tr:               3.00E-05	 1.00E-05	   10	    16
# fiqa:                  3.00E-05	 1.00E-06	   10	    16
# nfcorpus:              5.00E-06	 1.00E-06	    5	    32	

# cosqa_tr:              3.00E-05	 1.00E-06	   10	    32
# stack_overflow:        3.00E-05	 1.00E-06	   10	    16
# code_search:           3.00E-05	 1.00E-06	   10	    16

# news_cat:              3.00E-05	 1.00E-05	   10	    16
# bil_tweet_news:        3.00E-05	 1.00E-06	   10	    16
# gender_hate_scpeech:   2.00E-05	 1.00E-06	    3	    16
# prod_review:           2.00E-05	 1.00E-06	    1	    32
# pubmed_rct:            2.00E-05	 1.00E-06	    4	    16
# sci_cite_TR:           2.00E-05	 1.00E-05	    2	    32
# thesis_abstract:       3.00E-05	 1.00E-05	    5	    16

# wiki_ner:              2.00E-05	 1.00E-05	    3	    16
# wiki_ann_ner:          2.00E-05	 1.00E-05	   10	    16
# pos_ud_boun:           2.00E-05	 1.00E-05	    3	    32
# pos_ud_imst:           3.00E-05	 1.00E-06	   10	    16

# sick_tr:               3.00E-05	 1.00E-06	    5	    16
# sts:                   3.00E-05	 1.00E-06	    5	    16

# multinli:              1.00E-05	 1.00E-06	    3	    16
# snli:                  3.00E-05	 1.00E-06	    6	    32
# med_nli:               2.00E-05	 1.00E-05	    6	    16

# tquad:                 2.00E-05    1.00E-06	    2	    16
# xquad:                 3.00E-05	 1.00E-05	    7	    32